{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 793 training samples and 207 validation samples\n",
      "\n",
      "Feature statistics:\n",
      "        feature_0   feature_1   feature_2   feature_3      target\n",
      "count  793.000000  793.000000  793.000000  793.000000  793.000000\n",
      "mean     0.015966   -0.006794    0.017763   -0.002376    1.543506\n",
      "std      0.991243    1.017013    0.998462    1.026261    0.829346\n",
      "min     -1.823993   -1.676095   -4.557125   -0.936869    0.000000\n",
      "25%     -0.841765   -0.699470   -0.588484   -0.625870    2.000000\n",
      "50%      0.065698   -0.197729    0.020473   -0.391615    2.000000\n",
      "75%      0.931161    0.365627    0.598809    0.266843    2.000000\n",
      "max      1.768643    8.748089    4.253251   10.146557    2.000000\n",
      "\n",
      "Class distribution in training set:\n",
      "target\n",
      "2    0.762926\n",
      "0    0.219420\n",
      "1    0.017654\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "def generate_synthetic_data(n_samples=1000, n_features=4, n_classes=3, test_size=0.2, random_state=42):\n",
    "    \"\"\"Generate synthetic data for classification with some non-linear relationships.\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate features\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    # Create non-linear relationships\n",
    "    X[:, 0] = np.sin(X[:, 0]) + np.random.randn(n_samples) * 0.1\n",
    "    X[:, 1] = np.exp(X[:, 1] / 2) + np.random.randn(n_samples) * 0.1\n",
    "    X[:, 2] = X[:, 0] * X[:, 1] + np.random.randn(n_samples) * 0.1\n",
    "    X[:, 3] = np.square(X[:, 3]) + np.random.randn(n_samples) * 0.1\n",
    "    \n",
    "    # Generate target classes based on non-linear combinations\n",
    "    logits = np.zeros((n_samples, n_classes))\n",
    "    logits[:, 0] = np.sin(X[:, 0]) + np.cos(X[:, 1])\n",
    "    logits[:, 1] = X[:, 2] * X[:, 3] - np.square(X[:, 1])\n",
    "    logits[:, 2] = np.exp(X[:, 0]/2) - np.sin(X[:, 2] * X[:, 3])\n",
    "    \n",
    "    # Convert to probabilities and select class\n",
    "    probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "    y = np.argmax(probs, axis=1)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Create DataFrames\n",
    "    feature_cols = [f'feature_{i}' for i in range(n_features)]\n",
    "    df = pd.DataFrame(X, columns=feature_cols)\n",
    "    df['target'] = y\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    mask = np.random.rand(len(df)) < (1 - test_size)\n",
    "    train_df = df[mask]\n",
    "    val_df = df[~mask]\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "def main():\n",
    "    # Create input_data directory if it doesn't exist\n",
    "    os.makedirs('input_data', exist_ok=True)\n",
    "    \n",
    "    # Generate data\n",
    "    train_df, val_df = generate_synthetic_data(\n",
    "        n_samples=1000,\n",
    "        n_features=4,\n",
    "        n_classes=3,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Save to CSV files\n",
    "    train_df.to_csv('input_data/train.csv', index=False)\n",
    "    val_df.to_csv('input_data/val.csv', index=False)\n",
    "    \n",
    "    print(f\"Generated {len(train_df)} training samples and {len(val_df)} validation samples\")\n",
    "    print(\"\\nFeature statistics:\")\n",
    "    print(train_df.describe())\n",
    "    print(\"\\nClass distribution in training set:\")\n",
    "    print(train_df['target'].value_counts(normalize=True))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "import psutil\n",
    "import cpuinfo\n",
    "import intel_extension_for_pytorch as ipex\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 04:52:02,437] A new study created in memory with name: no-name-827af9f8-5f2b-48c7-826c-a8cd0d656d7e\n",
      "[I 2025-01-06 04:52:03,568] Trial 0 finished with value: 32.5 and parameters: {'n_layers': 4, 'hidden_layer_0': 349, 'hidden_layer_1': 217, 'hidden_layer_2': 111, 'hidden_layer_3': 253, 'lr': 0.0001528846969423591, 'dropout_rate': 0.25632740921438174, 'use_batch_norm': False, 'weight_decay': 0.00023685769920732717}. Best is trial 0 with value: 32.5.\n",
      "[I 2025-01-06 04:52:04,436] Trial 1 finished with value: 33.0 and parameters: {'n_layers': 4, 'hidden_layer_0': 262, 'hidden_layer_1': 492, 'hidden_layer_2': 156, 'hidden_layer_3': 104, 'lr': 0.07340111719517645, 'dropout_rate': 0.29457323840596084, 'use_batch_norm': False, 'weight_decay': 0.0018455239279866163}. Best is trial 1 with value: 33.0.\n",
      "[I 2025-01-06 04:52:04,843] Trial 2 pruned. Trial pruned due to significant metric deterioration\n",
      "[I 2025-01-06 04:52:06,080] Trial 3 finished with value: 29.5 and parameters: {'n_layers': 4, 'hidden_layer_0': 406, 'hidden_layer_1': 164, 'hidden_layer_2': 149, 'hidden_layer_3': 285, 'lr': 0.009274805614783173, 'dropout_rate': 0.4223147529729111, 'use_batch_norm': True}. Best is trial 1 with value: 33.0.\n",
      "[I 2025-01-06 04:52:06,730] Trial 4 finished with value: 26.5 and parameters: {'n_layers': 3, 'hidden_layer_0': 241, 'hidden_layer_1': 217, 'hidden_layer_2': 313, 'lr': 0.0006308499548459461, 'dropout_rate': 0.2685679911506431, 'use_batch_norm': True}. Best is trial 1 with value: 33.0.\n",
      "[I 2025-01-06 04:52:07,609] Trial 5 finished with value: 27.0 and parameters: {'n_layers': 4, 'hidden_layer_0': 305, 'hidden_layer_1': 412, 'hidden_layer_2': 124, 'hidden_layer_3': 499, 'lr': 0.00012660626685582859, 'dropout_rate': 0.4616339961010528, 'use_batch_norm': False, 'weight_decay': 0.0027305255370818674}. Best is trial 1 with value: 33.0.\n",
      "[I 2025-01-06 04:52:08,142] Trial 6 finished with value: 31.0 and parameters: {'n_layers': 1, 'hidden_layer_0': 315, 'lr': 0.08335319635323946, 'dropout_rate': 0.18916032602107324, 'use_batch_norm': True}. Best is trial 1 with value: 33.0.\n",
      "[I 2025-01-06 04:52:09,418] Trial 7 pruned. Trial pruned due to significant metric deterioration\n",
      "[I 2025-01-06 04:52:09,962] Trial 8 finished with value: 22.5 and parameters: {'n_layers': 2, 'hidden_layer_0': 94, 'hidden_layer_1': 452, 'lr': 3.4508160382333894e-05, 'dropout_rate': 0.47523959035966834, 'use_batch_norm': True}. Best is trial 1 with value: 33.0.\n",
      "[I 2025-01-06 04:52:10,410] Trial 9 finished with value: 27.0 and parameters: {'n_layers': 1, 'hidden_layer_0': 166, 'lr': 0.013678125978462175, 'dropout_rate': 0.13014419617975534, 'use_batch_norm': False, 'weight_decay': 1.2603274140824988e-05}. Best is trial 1 with value: 33.0.\n",
      "[I 2025-01-06 04:52:10,883] Trial 10 pruned. Trial pruned due to significant metric deterioration\n",
      "[I 2025-01-06 04:52:11,678] Trial 11 finished with value: 32.5 and parameters: {'n_layers': 3, 'hidden_layer_0': 402, 'hidden_layer_1': 307, 'hidden_layer_2': 35, 'lr': 0.0037856665369642556, 'dropout_rate': 0.2703551127398896, 'use_batch_norm': False, 'weight_decay': 0.00047424931738994857}. Best is trial 1 with value: 33.0.\n",
      "[I 2025-01-06 04:52:12,803] Trial 12 finished with value: 36.5 and parameters: {'n_layers': 4, 'hidden_layer_0': 177, 'hidden_layer_1': 487, 'hidden_layer_2': 217, 'hidden_layer_3': 32, 'lr': 1.0397486776002072e-05, 'dropout_rate': 0.20115280210708425, 'use_batch_norm': False, 'weight_decay': 0.00027323564506194205}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:13,336] Trial 13 finished with value: 23.5 and parameters: {'n_layers': 4, 'hidden_layer_0': 202, 'hidden_layer_1': 492, 'hidden_layer_2': 262, 'hidden_layer_3': 34, 'lr': 1.0650509664795874e-05, 'dropout_rate': 0.20654642049001826, 'use_batch_norm': False, 'weight_decay': 0.000708665309605546}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:13,660] Trial 14 pruned. Trial pruned due to significant metric deterioration\n",
      "[I 2025-01-06 04:52:14,419] Trial 15 finished with value: 19.0 and parameters: {'n_layers': 4, 'hidden_layer_0': 231, 'hidden_layer_1': 369, 'hidden_layer_2': 379, 'hidden_layer_3': 50, 'lr': 1.3333783783675336e-05, 'dropout_rate': 0.1966692857299035, 'use_batch_norm': False, 'weight_decay': 0.0017956045116615594}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:15,069] Trial 16 finished with value: 27.0 and parameters: {'n_layers': 2, 'hidden_layer_0': 125, 'hidden_layer_1': 429, 'lr': 0.001428632289550659, 'dropout_rate': 0.30902983934447686, 'use_batch_norm': False, 'weight_decay': 0.0013462338194913611}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:16,604] Trial 17 finished with value: 33.5 and parameters: {'n_layers': 3, 'hidden_layer_0': 195, 'hidden_layer_1': 510, 'hidden_layer_2': 224, 'lr': 0.014442247520923147, 'dropout_rate': 0.23211825327172503, 'use_batch_norm': False, 'weight_decay': 8.246221115455247e-05}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:17,163] Trial 18 finished with value: 33.0 and parameters: {'n_layers': 3, 'hidden_layer_0': 71, 'hidden_layer_1': 329, 'hidden_layer_2': 339, 'lr': 0.023878530457615173, 'dropout_rate': 0.22464690989533384, 'use_batch_norm': False, 'weight_decay': 7.293379952608958e-05}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:17,594] Trial 19 finished with value: 27.5 and parameters: {'n_layers': 3, 'hidden_layer_0': 191, 'hidden_layer_1': 418, 'hidden_layer_2': 215, 'lr': 3.927174970826337e-05, 'dropout_rate': 0.15750691130470912, 'use_batch_norm': False, 'weight_decay': 2.8619861370259043e-05}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:17,908] Trial 20 finished with value: 29.0 and parameters: {'n_layers': 2, 'hidden_layer_0': 118, 'hidden_layer_1': 53, 'lr': 0.0019829967184713084, 'dropout_rate': 0.105718438896606, 'use_batch_norm': False, 'weight_decay': 0.00018341498102665083}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:18,334] Trial 21 pruned. Trial pruned due to significant metric deterioration\n",
      "[I 2025-01-06 04:52:19,384] Trial 22 finished with value: 31.5 and parameters: {'n_layers': 4, 'hidden_layer_0': 203, 'hidden_layer_1': 457, 'hidden_layer_2': 67, 'hidden_layer_3': 168, 'lr': 0.006586521612999011, 'dropout_rate': 0.29767291772887045, 'use_batch_norm': False, 'weight_decay': 8.759950515437867e-05}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:20,318] Trial 23 finished with value: 33.0 and parameters: {'n_layers': 3, 'hidden_layer_0': 353, 'hidden_layer_1': 386, 'hidden_layer_2': 161, 'lr': 0.02411644186333042, 'dropout_rate': 0.23173568896751134, 'use_batch_norm': False, 'weight_decay': 0.006226573300037809}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:21,054] Trial 24 finished with value: 28.0 and parameters: {'n_layers': 4, 'hidden_layer_0': 171, 'hidden_layer_1': 470, 'hidden_layer_2': 290, 'hidden_layer_3': 136, 'lr': 0.09214451424621661, 'dropout_rate': 0.1651124298669167, 'use_batch_norm': False, 'weight_decay': 0.0003188256165076385}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:22,174] Trial 25 finished with value: 34.0 and parameters: {'n_layers': 3, 'hidden_layer_0': 227, 'hidden_layer_1': 508, 'hidden_layer_2': 221, 'lr': 0.01817712914686431, 'dropout_rate': 0.29289789879248096, 'use_batch_norm': False, 'weight_decay': 0.0010196570928820993}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:22,857] Trial 26 finished with value: 36.5 and parameters: {'n_layers': 3, 'hidden_layer_0': 218, 'hidden_layer_1': 449, 'hidden_layer_2': 212, 'lr': 0.01871994830368765, 'dropout_rate': 0.37319694811655457, 'use_batch_norm': True}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:23,246] Trial 27 finished with value: 25.0 and parameters: {'n_layers': 2, 'hidden_layer_0': 226, 'hidden_layer_1': 285, 'lr': 0.0003620906430002764, 'dropout_rate': 0.41528265554277416, 'use_batch_norm': True}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:23,790] Trial 28 pruned. Trial pruned due to significant metric deterioration\n",
      "[I 2025-01-06 04:52:25,307] Trial 29 finished with value: 27.5 and parameters: {'n_layers': 3, 'hidden_layer_0': 351, 'hidden_layer_1': 399, 'hidden_layer_2': 261, 'lr': 0.0002726485153821301, 'dropout_rate': 0.3595035166432504, 'use_batch_norm': True}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:26,243] Trial 30 finished with value: 29.0 and parameters: {'n_layers': 2, 'hidden_layer_0': 299, 'hidden_layer_1': 344, 'lr': 0.03255350396234345, 'dropout_rate': 0.33119593655321505, 'use_batch_norm': True}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:27,318] Trial 31 finished with value: 33.0 and parameters: {'n_layers': 3, 'hidden_layer_0': 213, 'hidden_layer_1': 511, 'hidden_layer_2': 207, 'lr': 0.01471332880203824, 'dropout_rate': 0.2517066171925859, 'use_batch_norm': False, 'weight_decay': 0.0008034242801848911}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:29,081] Trial 32 finished with value: 29.5 and parameters: {'n_layers': 3, 'hidden_layer_0': 179, 'hidden_layer_1': 471, 'hidden_layer_2': 236, 'lr': 0.010860786716897539, 'dropout_rate': 0.29385598191812395, 'use_batch_norm': True}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:29,745] Trial 33 pruned. Trial pruned due to significant metric deterioration\n",
      "[I 2025-01-06 04:52:31,327] Trial 34 finished with value: 30.5 and parameters: {'n_layers': 3, 'hidden_layer_0': 241, 'hidden_layer_1': 510, 'hidden_layer_2': 287, 'lr': 0.0027497023569683435, 'dropout_rate': 0.21794813517915687, 'use_batch_norm': True}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:32,380] Trial 35 finished with value: 34.0 and parameters: {'n_layers': 4, 'hidden_layer_0': 493, 'hidden_layer_1': 437, 'hidden_layer_2': 182, 'hidden_layer_3': 276, 'lr': 0.018543090139793054, 'dropout_rate': 0.2740865186727501, 'use_batch_norm': False, 'weight_decay': 0.00012805944008936667}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:33,341] Trial 36 finished with value: 25.0 and parameters: {'n_layers': 4, 'hidden_layer_0': 495, 'hidden_layer_1': 426, 'hidden_layer_2': 182, 'hidden_layer_3': 298, 'lr': 5.2861846897451386e-05, 'dropout_rate': 0.2828127334850505, 'use_batch_norm': True}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:34,004] Trial 37 finished with value: 33.5 and parameters: {'n_layers': 4, 'hidden_layer_0': 430, 'hidden_layer_1': 382, 'hidden_layer_2': 117, 'hidden_layer_3': 377, 'lr': 0.007214608107227027, 'dropout_rate': 0.4392138358939872, 'use_batch_norm': False, 'weight_decay': 0.0003665734964562708}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:36,417] Trial 38 finished with value: 29.0 and parameters: {'n_layers': 4, 'hidden_layer_0': 511, 'hidden_layer_1': 234, 'hidden_layer_2': 192, 'hidden_layer_3': 224, 'lr': 0.0009255144658175224, 'dropout_rate': 0.3230619803171863, 'use_batch_norm': True}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:37,228] Trial 39 finished with value: 27.0 and parameters: {'n_layers': 4, 'hidden_layer_0': 326, 'hidden_layer_1': 440, 'hidden_layer_2': 251, 'hidden_layer_3': 340, 'lr': 9.343281319335328e-05, 'dropout_rate': 0.2673789004730725, 'use_batch_norm': False, 'weight_decay': 0.00013829879491524932}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:37,417] Trial 40 pruned. Trial pruned due to significant metric deterioration\n",
      "[I 2025-01-06 04:52:38,423] Trial 41 finished with value: 32.5 and parameters: {'n_layers': 3, 'hidden_layer_0': 255, 'hidden_layer_1': 473, 'hidden_layer_2': 232, 'lr': 0.015011693884448648, 'dropout_rate': 0.24730761755474406, 'use_batch_norm': False, 'weight_decay': 2.06085567683037e-05}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:38,911] Trial 42 finished with value: 33.0 and parameters: {'n_layers': 4, 'hidden_layer_0': 154, 'hidden_layer_1': 489, 'hidden_layer_2': 149, 'hidden_layer_3': 201, 'lr': 0.05674068560267153, 'dropout_rate': 0.4960556472463997, 'use_batch_norm': False, 'weight_decay': 9.187842581324213e-05}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:39,445] Trial 43 finished with value: 31.5 and parameters: {'n_layers': 3, 'hidden_layer_0': 278, 'hidden_layer_1': 410, 'hidden_layer_2': 172, 'lr': 0.0051861183891396935, 'dropout_rate': 0.31358075074275094, 'use_batch_norm': False, 'weight_decay': 0.0002683782337892405}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:40,053] Trial 44 finished with value: 33.0 and parameters: {'n_layers': 4, 'hidden_layer_0': 381, 'hidden_layer_1': 483, 'hidden_layer_2': 83, 'hidden_layer_3': 400, 'lr': 0.009301982623858318, 'dropout_rate': 0.2815721562623216, 'use_batch_norm': False, 'weight_decay': 0.0009179606717493299}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:41,677] Trial 45 finished with value: 34.5 and parameters: {'n_layers': 3, 'hidden_layer_0': 120, 'hidden_layer_1': 451, 'hidden_layer_2': 323, 'lr': 0.017661307880061223, 'dropout_rate': 0.23219942453994655, 'use_batch_norm': False, 'weight_decay': 0.00019583605074653104}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:42,265] Trial 46 finished with value: 33.0 and parameters: {'n_layers': 3, 'hidden_layer_0': 95, 'hidden_layer_1': 86, 'hidden_layer_2': 344, 'lr': 0.031315137949080855, 'dropout_rate': 0.14360896741742374, 'use_batch_norm': False, 'weight_decay': 0.00017803864251332674}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:43,139] Trial 47 finished with value: 33.0 and parameters: {'n_layers': 2, 'hidden_layer_0': 163, 'hidden_layer_1': 445, 'lr': 0.0190228837947371, 'dropout_rate': 0.19296131339093453, 'use_batch_norm': False, 'weight_decay': 0.0031548860364315743}. Best is trial 12 with value: 36.5.\n",
      "[I 2025-01-06 04:52:43,836] Trial 48 pruned. Trial pruned due to significant metric deterioration\n",
      "[I 2025-01-06 04:52:44,967] Trial 49 finished with value: 27.5 and parameters: {'n_layers': 4, 'hidden_layer_0': 44, 'hidden_layer_1': 146, 'hidden_layer_2': 442, 'hidden_layer_3': 459, 'lr': 2.080237545535776e-05, 'dropout_rate': 0.2122153999937135, 'use_batch_norm': True}. Best is trial 12 with value: 36.5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model parameters from config:\n",
      "    best_metric_name: f1_score\n",
      "    best_metric_value: 36.5\n",
      "    dropout_rate: 0.20115280210708425\n",
      "    hidden_layers: [177, 487, 217, 32]\n",
      "    learning_rate: 1.0397486776002072e-05\n",
      "    use_batch_norm: False\n",
      "    weight_decay: 0.00027323564506194205\n",
      "\n",
      "Restoring best model from checkpoint...\n",
      "\n",
      "Evaluating restored model on validation set...\n",
      "\n",
      "Restored model performance:\n",
      "Validation Loss: 1.3803\n",
      "Validation Accuracy: 36.50%\n",
      "Validation F1-Score: 0.2620\n",
      "\n",
      "Best F1_SCORE from tuning: 36.5000\n",
      "Current F1_SCORE: 36.5000\n"
     ]
    }
   ],
   "source": [
    "def load_config(config_path):\n",
    "    \"\"\"Load configuration from a YAML file.\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "        \n",
    "# Set up logging\n",
    "def setup_logger(name='MLPTrainer'):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # File handler\n",
    "    fh = logging.FileHandler(f'{name}.log')\n",
    "    fh.setLevel(logging.INFO)\n",
    "    \n",
    "    # Console handler\n",
    "    ch = logging.StreamHandler(sys.stdout)\n",
    "    ch.setLevel(logging.INFO)\n",
    "    \n",
    "    # Formatter\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fh.setFormatter(formatter)\n",
    "    ch.setFormatter(formatter)\n",
    "    \n",
    "    # Add handlers to logger\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, target_column):\n",
    "        self.features = torch.FloatTensor(df.drop(target_column, axis=1).values)\n",
    "        self.labels = torch.LongTensor(df[target_column].values)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, num_classes=3, dropout_rate=0.2, use_batch_norm=True):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(prev_size, num_classes))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "class PyTorchTrainer:\n",
    "    \"\"\"A generic PyTorch trainer class.\n",
    "    \n",
    "    Attributes:\n",
    "        model: PyTorch model to train\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimization algorithm\n",
    "        device: Device to train on (CPU/GPU)\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, criterion, optimizer, device='cpu', verbose=False):\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"Trains the model for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(batch_X)\n",
    "            loss = self.criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            # Save the best model and optimizer from hyperparameter tuning. Reload this best model after hyperparameter tuning. apply the reloaded model to the validation dataset as the final step, to compare its performance with the results of the train_final_model step.\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        return total_loss / len(train_loader), accuracy\n",
    "    \n",
    "    def evaluate(self, val_loader):\n",
    "        \"\"\"Evaluates the model on validation data.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = self.criterion(outputs, batch_y)\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        return total_loss / len(val_loader), accuracy, f1\n",
    "\n",
    "    def train(self, train_loader, val_loader, epochs, metric='accuracy'):\n",
    "        \"\"\"Trains the model for specified number of epochs. \n",
    "        Monitors specified validation metric for early stopping.\"\"\"\n",
    "        train_losses, val_losses = [], []\n",
    "        train_metrics, val_metrics = [], []\n",
    "        best_val_metric = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs), desc='Training'):\n",
    "            train_loss, train_accuracy = self.train_epoch(train_loader)\n",
    "            val_loss, val_accuracy, val_f1 = self.evaluate(val_loader)\n",
    "            \n",
    "            # Select metric based on config\n",
    "            train_metric = train_accuracy\n",
    "            val_metric = val_f1 if metric == 'f1' else val_accuracy\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_metrics.append(train_metric)\n",
    "            val_metrics.append(val_metric)\n",
    "            \n",
    "            best_val_metric = max(best_val_metric, val_metric)\n",
    "            \n",
    "            if self.verbose:\n",
    "                metric_name = 'F1' if metric == 'f1' else 'Accuracy'\n",
    "                metric_value = val_f1 if metric == 'f1' else val_accuracy\n",
    "                print(f'Epoch {epoch+1}/{epochs}: Val {metric_name}: {metric_value:.2f}%')\n",
    "        \n",
    "        self.plot_learning_curves(train_losses, val_losses, train_metrics, val_metrics, \n",
    "                                metric_name='F1-Score' if metric == 'f1' else 'Accuracy')\n",
    "        \n",
    "        return train_losses, val_losses, train_metrics, val_metrics, best_val_metric\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_learning_curves(train_losses, val_losses, train_metrics, val_metrics, metric_name='Accuracy'):\n",
    "        \"\"\"Plots the learning curves for loss and chosen metric (accuracy or F1).\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        \n",
    "        # Normalize values for better visualization\n",
    "        max_loss = max(max(train_losses), max(val_losses))\n",
    "        max_metric = max(max(train_metrics), max(val_metrics))\n",
    "        \n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "        \n",
    "        sns.lineplot(data={\n",
    "            f\"Training {metric_name}\": [x/max_metric for x in train_metrics],\n",
    "            f\"Validation {metric_name}\": [x/max_metric for x in val_metrics],\n",
    "            \"Training Loss\": [x/max_loss for x in train_losses],\n",
    "            \"Validation Loss\": [x/max_loss for x in val_losses]\n",
    "        })\n",
    "        \n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Normalized Value\")\n",
    "        plt.title(f\"Training and Validation Loss and {metric_name} Curves\")\n",
    "        plt.legend()\n",
    "        plt.savefig('learning_curves.png')\n",
    "        plt.close()\n",
    "\n",
    "class HyperparameterTuner:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.best_trial_value = float('-inf')\n",
    "        self.best_model_state = None\n",
    "        self.best_optimizer_state = None\n",
    "        self.best_params = None\n",
    "        os.makedirs(os.path.dirname(config['model']['save_path']), exist_ok=True)\n",
    "    \n",
    "    def save_best_model(self, model, optimizer, trial_value, params):\n",
    "        \"\"\"Save the best model and its metadata.\"\"\"\n",
    "        checkpoint = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'optimizer_name': self.config['training']['optimizer_choice'],\n",
    "            'metric_name': self.config['training']['optimization_metric'],\n",
    "            'metric_value': trial_value,\n",
    "            'hyperparameters': params\n",
    "        }\n",
    "        torch.save(checkpoint, self.config['model']['save_path'])\n",
    "    \n",
    "    def create_model_and_optimizer(self, trial):\n",
    "        # Extract hyperparameters from trial\n",
    "        hidden_layers = []\n",
    "        n_layers = trial.suggest_int('n_layers', 1, 4)\n",
    "        for i in range(n_layers):\n",
    "            hidden_layers.append(trial.suggest_int(f'hidden_layer_{i}', 32, 512))\n",
    "        \n",
    "        lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "        use_batch_norm = trial.suggest_categorical('use_batch_norm', [True, False])\n",
    "        weight_decay = 0.0 if use_batch_norm else trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "        \n",
    "        # Create model\n",
    "        model = MLPClassifier(\n",
    "            input_size=self.config['model']['input_size'],\n",
    "            hidden_layers=hidden_layers,\n",
    "            num_classes=self.config['model']['num_classes'],\n",
    "            dropout_rate=dropout_rate,\n",
    "            use_batch_norm=use_batch_norm\n",
    "        )\n",
    "        \n",
    "        # Create optimizer\n",
    "        optimizer = getattr(torch.optim, self.config['training']['optimizer_choice'])(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        \n",
    "        trial_params = {\n",
    "            'n_layers': n_layers,\n",
    "            'hidden_layers': hidden_layers,\n",
    "            'lr': lr,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'use_batch_norm': use_batch_norm,\n",
    "            'weight_decay': weight_decay\n",
    "        }\n",
    "        \n",
    "        return model, optimizer, trial_params\n",
    "    \n",
    "    def objective(self, trial, train_loader, val_loader):\n",
    "        model, optimizer, trial_params = self.create_model_and_optimizer(trial)\n",
    "        criterion = getattr(nn, self.config['training']['loss_function'])()\n",
    "        \n",
    "        trainer = PyTorchTrainer(\n",
    "            model, criterion, optimizer,\n",
    "            device=self.config['training']['device']\n",
    "        )\n",
    "        \n",
    "        patience = self.config['optimization']['early_stopping']['patience']\n",
    "        min_delta = self.config['optimization']['early_stopping']['min_delta']\n",
    "        best_metric = float('-inf')\n",
    "        patience_counter = 0\n",
    "        last_metric = float('-inf')\n",
    "        \n",
    "        # Add warm-up period\n",
    "        warm_up_epochs = 3\n",
    "        running_metrics = []\n",
    "        \n",
    "        for epoch in range(self.config['training']['epochs']):\n",
    "            trainer.train_epoch(train_loader)\n",
    "            _, accuracy, f1 = trainer.evaluate(val_loader)\n",
    "            \n",
    "            metric = f1 if self.config['training']['optimization_metric'] == 'f1' else accuracy\n",
    "            trial.report(metric, epoch)\n",
    "            \n",
    "            running_metrics.append(metric)\n",
    "            if len(running_metrics) > 3:\n",
    "                running_metrics.pop(0)\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if metric > best_metric + min_delta:\n",
    "                best_metric = metric\n",
    "                patience_counter = 0\n",
    "                \n",
    "                if metric > self.best_trial_value:\n",
    "                    self.best_trial_value = metric\n",
    "                    self.save_best_model(model, optimizer, metric, trial_params)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Modified pruning logic with warm-up and relative threshold\n",
    "            if epoch >= warm_up_epochs:\n",
    "                avg_metric = sum(running_metrics) / len(running_metrics)\n",
    "                relative_deterioration = (best_metric - avg_metric) / (best_metric + 1e-8)\n",
    "                \n",
    "                if relative_deterioration > 0.3:  # 30% deterioration threshold\n",
    "                    raise optuna.TrialPruned(\"Trial pruned due to significant metric deterioration\")\n",
    "            \n",
    "            last_metric = metric\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "        \n",
    "        return best_metric\n",
    "    \n",
    "    def tune(self, train_loader, val_loader):\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            pruner=optuna.pruners.MedianPruner()\n",
    "        )\n",
    "        \n",
    "        study.optimize(\n",
    "            lambda trial: self.objective(trial, train_loader, val_loader),\n",
    "            n_trials=self.config['optimization']['n_trials']\n",
    "        )\n",
    "        \n",
    "        return study.best_trial, study.best_params\n",
    "\n",
    "def restore_best_model(config):\n",
    "    \"\"\"Utility function to restore the best model and its optimizer.\"\"\"\n",
    "    checkpoint = torch.load(config['model']['save_path'], weights_only=True)\n",
    "    \n",
    "    # Create model with saved hyperparameters\n",
    "    model = MLPClassifier(\n",
    "        input_size=config['model']['input_size'],\n",
    "        hidden_layers=checkpoint['hyperparameters']['hidden_layers'],\n",
    "        num_classes=config['model']['num_classes'],\n",
    "        dropout_rate=checkpoint['hyperparameters']['dropout_rate'],\n",
    "        use_batch_norm=checkpoint['hyperparameters']['use_batch_norm']\n",
    "    )\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = getattr(torch.optim, checkpoint['optimizer_name'])(\n",
    "        model.parameters(),\n",
    "        lr=checkpoint['hyperparameters']['lr'],\n",
    "        weight_decay=checkpoint['hyperparameters'].get('weight_decay', 0.0)\n",
    "    )\n",
    "    \n",
    "    # Load states\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'optimizer': optimizer,\n",
    "        'metric_name': checkpoint['metric_name'],\n",
    "        'metric_value': checkpoint['metric_value'],\n",
    "        'hyperparameters': checkpoint['hyperparameters']\n",
    "    }\n",
    "\n",
    "def save_best_params_to_config(config_path, best_trial, best_params):\n",
    "    \"\"\"Save best parameters to config file.\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Create best_model section if it doesn't exist\n",
    "    if 'best_model' not in config:\n",
    "        config['best_model'] = {}\n",
    "    \n",
    "    # Format parameters for config\n",
    "    hidden_layers = [best_params[f'hidden_layer_{i}'] for i in range(best_params['n_layers'])]\n",
    "    \n",
    "    config['best_model'].update({\n",
    "        'hidden_layers': hidden_layers,\n",
    "        'dropout_rate': best_params['dropout_rate'],\n",
    "        'learning_rate': best_params['lr'],\n",
    "        'use_batch_norm': best_params['use_batch_norm'],\n",
    "        'weight_decay': best_params.get('weight_decay', 0.0),\n",
    "        'best_metric_name': config['training']['optimization_metric'],\n",
    "        'best_metric_value': best_trial.value\n",
    "    })\n",
    "    \n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "def train_final_model(config, train_loader, val_loader):\n",
    "    \"\"\"Train model using parameters from config.\"\"\"\n",
    "    best_model_config = config['best_model']\n",
    "    \n",
    "    final_model = MLPClassifier(\n",
    "        input_size=config['model']['input_size'],\n",
    "        hidden_layers=best_model_config['hidden_layers'],\n",
    "        num_classes=config['model']['num_classes'],\n",
    "        dropout_rate=best_model_config['dropout_rate'],\n",
    "        use_batch_norm=best_model_config['use_batch_norm']\n",
    "    )\n",
    "    \n",
    "    criterion = getattr(nn, config['training']['loss_function'])()\n",
    "    optimizer = getattr(torch.optim, config['training']['optimizer_choice'])(\n",
    "        final_model.parameters(),\n",
    "        lr=best_model_config['learning_rate'],\n",
    "        weight_decay=best_model_config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    final_trainer = PyTorchTrainer(\n",
    "        final_model, criterion, optimizer,\n",
    "        device=config['training']['device'],\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    return final_trainer.train(\n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        config['training']['epochs'],\n",
    "        metric=config['training']['optimization_metric']\n",
    "    )\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "class CPUOptimizer:\n",
    "    \"\"\"Handles CPU-specific optimizations for PyTorch training.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger('CPUOptimizer')\n",
    "        self.cpu_info = cpuinfo.get_cpu_info()\n",
    "        \n",
    "    def detect_cpu_features(self):\n",
    "        \"\"\"Detect CPU features and capabilities.\"\"\"\n",
    "        features = {\n",
    "            'processor': self.cpu_info.get('brand_raw', 'Unknown'),\n",
    "            'architecture': self.cpu_info.get('arch', 'Unknown'),\n",
    "            'cores': psutil.cpu_count(logical=False),\n",
    "            'threads': psutil.cpu_count(logical=True),\n",
    "            'avx512': 'avx512' in self.cpu_info.get('flags', []),\n",
    "            'avx2': 'avx2' in self.cpu_info.get('flags', []),\n",
    "            'mkl': hasattr(torch, 'backends') and hasattr(torch.backends, 'mkl') and torch.backends.mkl.is_available(),\n",
    "            'ipex': hasattr(torch, 'xpu') or hasattr(torch, 'ipex')\n",
    "        }\n",
    "        return features\n",
    "        \n",
    "    def configure_optimizations(self):\n",
    "        \"\"\"Configure CPU-specific optimizations based on detected features.\"\"\"\n",
    "        features = self.detect_cpu_features()\n",
    "        optimizations = {}\n",
    "        \n",
    "        # Configure number of threads\n",
    "        if self.config['training']['cpu_optimization']['num_threads'] == 'auto':\n",
    "            optimizations['num_threads'] = features['threads']\n",
    "        else:\n",
    "            optimizations['num_threads'] = self.config['training']['cpu_optimization']['num_threads']\n",
    "        \n",
    "        # Configure MKL-DNN\n",
    "        optimizations['enable_mkldnn'] = (\n",
    "            features['avx512'] or features['avx2']\n",
    "        ) and self.config['training']['cpu_optimization']['enable_mkldnn']\n",
    "        \n",
    "        # Configure data types\n",
    "        optimizations['use_bfloat16'] = (\n",
    "            features['avx512'] and \n",
    "            self.config['training']['cpu_optimization']['use_bfloat16']\n",
    "        )\n",
    "        \n",
    "        # Set thread configurations\n",
    "        torch.set_num_threads(optimizations['num_threads'])\n",
    "        if hasattr(torch, 'set_num_interop_threads'):\n",
    "            torch.set_num_interop_threads(min(4, optimizations['num_threads']))\n",
    "        \n",
    "        # Enable MKL-DNN if available\n",
    "        if optimizations['enable_mkldnn']:\n",
    "            torch.backends.mkldnn.enabled = True\n",
    "        \n",
    "        self.log_optimization_config(features, optimizations)\n",
    "        return optimizations\n",
    "        \n",
    "    def log_optimization_config(self, features, optimizations):\n",
    "        \"\"\"Log CPU features and applied optimizations.\"\"\"\n",
    "        self.logger.info(\"CPU Configuration:\")\n",
    "        self.logger.info(f\"Processor: {features['processor']}\")\n",
    "        self.logger.info(f\"Architecture: {features['architecture']}\")\n",
    "        self.logger.info(f\"Physical cores: {features['cores']}\")\n",
    "        self.logger.info(f\"Logical threads: {features['threads']}\")\n",
    "        self.logger.info(\"\\nCPU Features:\")\n",
    "        self.logger.info(f\"AVX-512 support: {features['avx512']}\")\n",
    "        self.logger.info(f\"AVX2 support: {features['avx2']}\")\n",
    "        self.logger.info(f\"MKL support: {features['mkl']}\")\n",
    "        self.logger.info(f\"IPEX support: {features['ipex']}\")\n",
    "        self.logger.info(\"\\nApplied Optimizations:\")\n",
    "        self.logger.info(f\"Number of threads: {optimizations['num_threads']}\")\n",
    "        self.logger.info(f\"MKL-DNN enabled: {optimizations['enable_mkldnn']}\")\n",
    "        self.logger.info(f\"BFloat16 enabled: {optimizations['use_bfloat16']}\")\n",
    "\n",
    "def main():\n",
    "    config_path = 'config.yaml'\n",
    "    config = load_config(config_path)\n",
    "    \n",
    "    # Set up logging\n",
    "    setup_logger()\n",
    "    \n",
    "    # Initialize CPU optimization\n",
    "    cpu_optimizer = CPUOptimizer(config)\n",
    "    optimizations = cpu_optimizer.configure_optimizations()\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    set_seed(config['training']['seed'])\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_df = pd.read_csv(config['data']['train_path'])\n",
    "    val_df = pd.read_csv(config['data']['val_path'])\n",
    "    train_dataset = CustomDataset(train_df, config['data']['target_column'])\n",
    "    val_dataset = CustomDataset(val_df, config['data']['target_column'])\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # If best parameters don't exist in config, run hyperparameter tuning\n",
    "    if 'best_model' not in config:\n",
    "        tuner = HyperparameterTuner(config)\n",
    "        best_trial, best_params = tuner.tune(train_loader, val_loader)\n",
    "        save_best_params_to_config(config_path, best_trial, best_params)\n",
    "        # Reload config with saved parameters\n",
    "        config = load_config(config_path)\n",
    "    \n",
    "    print(\"\\nBest model parameters from config:\")\n",
    "    for key, value in config['best_model'].items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    # Restore best model from checkpoint\n",
    "    print(\"\\nRestoring best model from checkpoint...\")\n",
    "    restored = restore_best_model(config)\n",
    "    model = restored['model']\n",
    "    optimizer = restored['optimizer']\n",
    "    \n",
    "    # Create criterion for evaluation\n",
    "    criterion = getattr(nn, config['training']['loss_function'])()\n",
    "    \n",
    "    # Create trainer for evaluation\n",
    "    trainer = PyTorchTrainer(\n",
    "        model, criterion, optimizer,\n",
    "        device=config['training']['device'],\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Evaluate restored model\n",
    "    print(\"\\nEvaluating restored model on validation set...\")\n",
    "    val_loss, val_accuracy, val_f1 = trainer.evaluate(val_loader)\n",
    "    \n",
    "    metric_name = config['training']['optimization_metric']\n",
    "    metric_value = val_f1 if metric_name == 'f1' else val_accuracy\n",
    "    \n",
    "    print(f\"\\nRestored model performance:\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "    print(f\"Validation F1-Score: {val_f1:.4f}\")\n",
    "    print(f\"\\nBest {metric_name.upper()} from tuning: {restored['metric_value']:.4f}\")\n",
    "    print(f\"Current {metric_name.upper()}: {metric_value:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
