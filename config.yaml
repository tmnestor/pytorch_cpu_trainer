data:
  target_column: target
  train_path: input_data/train.csv
  val_path: input_data/val.csv

model:
  input_size: 4
  num_classes: 4
  save_path: checkpoints/model.pt

logging:
  directory: logs
  console_level: WARNING
  file_level: INFO
  handlers:
    hyperparameter_tuning:
      filename: hyperparameter_tuning.log
      level: INFO
    cpu_optimization:
      filename: cpu_optimization.log
      level: DEBUG
    mlptrainer:
      filename: mlptrainer.log
      level: INFO

training:
  device: cpu
  epochs: 100
  batch_size: 128
  seed: 42
  loss_function: CrossEntropyLoss
  optimization_metric: f1_score
  
  optimizer_choice: Adam
  optimizer_params:
    Adam:
      lr: 0.0001
      betas: [0.9, 0.999]
      eps: 1.0e-8

  performance:
    batch_size_multiplier: 1
    enable_mkldnn: true
    grad_accum_steps: 4
    memory_fraction: 0.95
    num_workers: 8
    persistent_workers: true
    pin_memory: false
    prefetch_factor: 2

  cpu_optimization:
    enable_mkldnn: true
    num_threads: auto
    use_bfloat16: true
    jit_compile: true

  dataloader:
    num_workers: auto
    persistent_workers: true
    prefetch_factor: 2
    pin_memory: false
    drop_last: true

  label_smoothing:
    enabled: true
    factor: 0.1

  memory_management:
    optimization:
      grad_accumulation:
        enabled: true
        max_steps: 16

  scheduler:
    type: OneCycleLR
    params:
      max_lr_factor: 10.0
      pct_start: 0.3
      div_factor: 25.0
      final_div_factor: 1e4

  swa:
    enabled: true
    start_epoch: 75
    lr: 0.001

  warmup:
    enabled: true
    max_steps: 1000
    initial_lr_factor: 0.01

optimization:
  n_trials: 50
  early_stopping:
    patience: 10
    min_delta: 0.001
  pruning:
    warm_up_epochs: 3
    deterioration_threshold: 0.3
    min_trials_complete: 5
