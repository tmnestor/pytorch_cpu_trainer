{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-06 04:38:26,330] A new study created in memory with name: no-name-aadb238a-87bd-43f6-ac49-cfa08667762a\n",
      "[I 2025-01-06 04:38:30,082] Trial 0 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:30,180] Trial 1 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:30,338] Trial 2 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:30,476] Trial 3 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:30,526] Trial 4 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:30,609] Trial 5 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:30,741] Trial 6 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:30,825] Trial 7 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:30,911] Trial 8 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:30,986] Trial 9 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:31,081] Trial 10 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:31,210] Trial 11 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:31,533] Trial 12 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:31,654] Trial 13 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:31,714] Trial 14 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:31,841] Trial 15 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:31,941] Trial 16 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:31,999] Trial 17 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:32,172] Trial 18 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:32,596] Trial 19 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:32,816] Trial 20 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:33,128] Trial 21 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:33,255] Trial 22 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:33,643] Trial 23 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:33,901] Trial 24 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:34,161] Trial 25 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:34,307] Trial 26 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:34,468] Trial 27 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:34,584] Trial 28 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:34,754] Trial 29 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:35,074] Trial 30 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:35,451] Trial 31 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:36,195] Trial 32 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:36,347] Trial 33 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:36,539] Trial 34 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:36,670] Trial 35 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:36,881] Trial 36 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:37,044] Trial 37 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:37,161] Trial 38 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:37,259] Trial 39 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:37,501] Trial 40 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:37,664] Trial 41 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:37,920] Trial 42 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:38,344] Trial 43 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:38,623] Trial 44 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:38,804] Trial 45 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:39,310] Trial 46 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:39,395] Trial 47 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:39,613] Trial 48 pruned. Trial pruned due to metric deterioration\n",
      "[I 2025-01-06 04:38:39,828] Trial 49 pruned. Trial pruned due to metric deterioration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def load_config(config_path):\n",
    "    \"\"\"Load configuration from a YAML file.\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "        \n",
    "# Set up logging\n",
    "def setup_logger(name='MLPTrainer'):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # File handler\n",
    "    fh = logging.FileHandler(f'{name}.log')\n",
    "    fh.setLevel(logging.INFO)\n",
    "    \n",
    "    # Console handler\n",
    "    ch = logging.StreamHandler(sys.stdout)\n",
    "    ch.setLevel(logging.INFO)\n",
    "    \n",
    "    # Formatter\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fh.setFormatter(formatter)\n",
    "    ch.setFormatter(formatter)\n",
    "    \n",
    "    # Add handlers to logger\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, target_column):\n",
    "        self.features = torch.FloatTensor(df.drop(target_column, axis=1).values)\n",
    "        self.labels = torch.LongTensor(df[target_column].values)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, num_classes=3, dropout_rate=0.2, use_batch_norm=True):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(prev_size, num_classes))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "class PyTorchTrainer:\n",
    "    \"\"\"A generic PyTorch trainer class.\n",
    "    \n",
    "    Attributes:\n",
    "        model: PyTorch model to train\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimization algorithm\n",
    "        device: Device to train on (CPU/GPU)\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, criterion, optimizer, device='cpu', verbose=False):\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"Trains the model for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(batch_X)\n",
    "            loss = self.criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            # Save the best model and optimizer from hyperparameter tuning. Reload this best model after hyperparameter tuning. apply the reloaded model to the validation dataset as the final step, to compare its performance with the results of the train_final_model step.\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        return total_loss / len(train_loader), accuracy\n",
    "    \n",
    "    def evaluate(self, val_loader):\n",
    "        \"\"\"Evaluates the model on validation data.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = self.criterion(outputs, batch_y)\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        return total_loss / len(val_loader), accuracy, f1\n",
    "\n",
    "    def train(self, train_loader, val_loader, epochs, metric='accuracy'):\n",
    "        \"\"\"Trains the model for specified number of epochs. \n",
    "        Monitors specified validation metric for early stopping.\"\"\"\n",
    "        train_losses, val_losses = [], []\n",
    "        train_metrics, val_metrics = [], []\n",
    "        best_val_metric = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs), desc='Training'):\n",
    "            train_loss, train_accuracy = self.train_epoch(train_loader)\n",
    "            val_loss, val_accuracy, val_f1 = self.evaluate(val_loader)\n",
    "            \n",
    "            # Select metric based on config\n",
    "            train_metric = train_accuracy\n",
    "            val_metric = val_f1 if metric == 'f1' else val_accuracy\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_metrics.append(train_metric)\n",
    "            val_metrics.append(val_metric)\n",
    "            \n",
    "            best_val_metric = max(best_val_metric, val_metric)\n",
    "            \n",
    "            if self.verbose:\n",
    "                metric_name = 'F1' if metric == 'f1' else 'Accuracy'\n",
    "                metric_value = val_f1 if metric == 'f1' else val_accuracy\n",
    "                print(f'Epoch {epoch+1}/{epochs}: Val {metric_name}: {metric_value:.2f}%')\n",
    "        \n",
    "        self.plot_learning_curves(train_losses, val_losses, train_metrics, val_metrics, \n",
    "                                metric_name='F1-Score' if metric == 'f1' else 'Accuracy')\n",
    "        \n",
    "        return train_losses, val_losses, train_metrics, val_metrics, best_val_metric\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_learning_curves(train_losses, val_losses, train_metrics, val_metrics, metric_name='Accuracy'):\n",
    "        \"\"\"Plots the learning curves for loss and chosen metric (accuracy or F1).\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        \n",
    "        # Normalize values for better visualization\n",
    "        max_loss = max(max(train_losses), max(val_losses))\n",
    "        max_metric = max(max(train_metrics), max(val_metrics))\n",
    "        \n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "        \n",
    "        sns.lineplot(data={\n",
    "            f\"Training {metric_name}\": [x/max_metric for x in train_metrics],\n",
    "            f\"Validation {metric_name}\": [x/max_metric for x in val_metrics],\n",
    "            \"Training Loss\": [x/max_loss for x in train_losses],\n",
    "            \"Validation Loss\": [x/max_loss for x in val_losses]\n",
    "        })\n",
    "        \n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Normalized Value\")\n",
    "        plt.title(f\"Training and Validation Loss and {metric_name} Curves\")\n",
    "        plt.legend()\n",
    "        plt.savefig('learning_curves.png')\n",
    "        plt.close()\n",
    "\n",
    "class HyperparameterTuner:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.best_trial_value = float('-inf')\n",
    "        self.best_model_state = None\n",
    "        self.best_optimizer_state = None\n",
    "        self.best_params = None\n",
    "        os.makedirs(os.path.dirname(config['model']['save_path']), exist_ok=True)\n",
    "    \n",
    "    def save_best_model(self, model, optimizer, trial_value, params):\n",
    "        \"\"\"Save the best model and its metadata.\"\"\"\n",
    "        checkpoint = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'optimizer_name': self.config['training']['optimizer_choice'],\n",
    "            'metric_name': self.config['training']['optimization_metric'],\n",
    "            'metric_value': trial_value,\n",
    "            'hyperparameters': params\n",
    "        }\n",
    "        torch.save(checkpoint, self.config['model']['save_path'])\n",
    "    \n",
    "    def create_model_and_optimizer(self, trial):\n",
    "        # Extract hyperparameters from trial\n",
    "        hidden_layers = []\n",
    "        n_layers = trial.suggest_int('n_layers', 1, 4)\n",
    "        for i in range(n_layers):\n",
    "            hidden_layers.append(trial.suggest_int(f'hidden_layer_{i}', 32, 512))\n",
    "        \n",
    "        lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "        use_batch_norm = trial.suggest_categorical('use_batch_norm', [True, False])\n",
    "        weight_decay = 0.0 if use_batch_norm else trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "        \n",
    "        # Create model\n",
    "        model = MLPClassifier(\n",
    "            input_size=self.config['model']['input_size'],\n",
    "            hidden_layers=hidden_layers,\n",
    "            num_classes=self.config['model']['num_classes'],\n",
    "            dropout_rate=dropout_rate,\n",
    "            use_batch_norm=use_batch_norm\n",
    "        )\n",
    "        \n",
    "        # Create optimizer\n",
    "        optimizer = getattr(torch.optim, self.config['training']['optimizer_choice'])(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        \n",
    "        trial_params = {\n",
    "            'n_layers': n_layers,\n",
    "            'hidden_layers': hidden_layers,\n",
    "            'lr': lr,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'use_batch_norm': use_batch_norm,\n",
    "            'weight_decay': weight_decay\n",
    "        }\n",
    "        \n",
    "        return model, optimizer, trial_params\n",
    "    \n",
    "    def objective(self, trial, train_loader, val_loader):\n",
    "        model, optimizer, trial_params = self.create_model_and_optimizer(trial)\n",
    "        criterion = getattr(nn, self.config['training']['loss_function'])()\n",
    "        \n",
    "        trainer = PyTorchTrainer(\n",
    "            model, criterion, optimizer,\n",
    "            device=self.config['training']['device']\n",
    "        )\n",
    "        \n",
    "        patience = self.config['optimization']['early_stopping']['patience']\n",
    "        min_delta = self.config['optimization']['early_stopping']['min_delta']\n",
    "        best_metric = float('-inf')\n",
    "        patience_counter = 0\n",
    "        last_metric = float('-inf')\n",
    "        \n",
    "        # Add warm-up period\n",
    "        warm_up_epochs = 3\n",
    "        running_metrics = []\n",
    "        \n",
    "        for epoch in range(self.config['training']['epochs']):\n",
    "            trainer.train_epoch(train_loader)\n",
    "            _, accuracy, f1 = trainer.evaluate(val_loader)\n",
    "            \n",
    "            metric = f1 if self.config['training']['optimization_metric'] == 'f1' else accuracy\n",
    "            trial.report(metric, epoch)\n",
    "            \n",
    "            running_metrics.append(metric)\n",
    "            if len(running_metrics) > 3:\n",
    "                running_metrics.pop(0)\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if metric > best_metric + min_delta:\n",
    "                best_metric = metric\n",
    "                patience_counter = 0\n",
    "                \n",
    "                if metric > self.best_trial_value:\n",
    "                    self.best_trial_value = metric\n",
    "                    self.save_best_model(model, optimizer, metric, trial_params)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Modified pruning logic with warm-up and relative threshold\n",
    "            if epoch >= warm_up_epochs:\n",
    "                avg_metric = sum(running_metrics) / len(running_metrics)\n",
    "                relative_deterioration = (best_metric - avg_metric) / (best_metric + 1e-8)\n",
    "                \n",
    "                if relative_deterioration > 0.3:  # 30% deterioration threshold\n",
    "                    raise optuna.TrialPruned(\"Trial pruned due to significant metric deterioration\")\n",
    "            \n",
    "            last_metric = metric\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "        \n",
    "        return best_metric\n",
    "    \n",
    "    def tune(self, train_loader, val_loader):\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            pruner=optuna.pruners.MedianPruner()\n",
    "        )\n",
    "        \n",
    "        study.optimize(\n",
    "            lambda trial: self.objective(trial, train_loader, val_loader),\n",
    "            n_trials=self.config['optimization']['n_trials']\n",
    "        )\n",
    "        \n",
    "        return study.best_trial, study.best_params\n",
    "\n",
    "def restore_best_model(config):\n",
    "    \"\"\"Utility function to restore the best model and its optimizer.\"\"\"\n",
    "    checkpoint = torch.load(config['model']['save_path'], weights_only=True)\n",
    "    \n",
    "    # Create model with saved hyperparameters\n",
    "    model = MLPClassifier(\n",
    "        input_size=config['model']['input_size'],\n",
    "        hidden_layers=checkpoint['hyperparameters']['hidden_layers'],\n",
    "        num_classes=config['model']['num_classes'],\n",
    "        dropout_rate=checkpoint['hyperparameters']['dropout_rate'],\n",
    "        use_batch_norm=checkpoint['hyperparameters']['use_batch_norm']\n",
    "    )\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = getattr(torch.optim, checkpoint['optimizer_name'])(\n",
    "        model.parameters(),\n",
    "        lr=checkpoint['hyperparameters']['lr'],\n",
    "        weight_decay=checkpoint['hyperparameters'].get('weight_decay', 0.0)\n",
    "    )\n",
    "    \n",
    "    # Load states\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'optimizer': optimizer,\n",
    "        'metric_name': checkpoint['metric_name'],\n",
    "        'metric_value': checkpoint['metric_value'],\n",
    "        'hyperparameters': checkpoint['hyperparameters']\n",
    "    }\n",
    "\n",
    "def save_best_params_to_config(config_path, best_trial, best_params):\n",
    "    \"\"\"Save best parameters to config file.\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Create best_model section if it doesn't exist\n",
    "    if 'best_model' not in config:\n",
    "        config['best_model'] = {}\n",
    "    \n",
    "    # Format parameters for config\n",
    "    hidden_layers = [best_params[f'hidden_layer_{i}'] for i in range(best_params['n_layers'])]\n",
    "    \n",
    "    config['best_model'].update({\n",
    "        'hidden_layers': hidden_layers,\n",
    "        'dropout_rate': best_params['dropout_rate'],\n",
    "        'learning_rate': best_params['lr'],\n",
    "        'use_batch_norm': best_params['use_batch_norm'],\n",
    "        'weight_decay': best_params.get('weight_decay', 0.0),\n",
    "        'best_metric_name': config['training']['optimization_metric'],\n",
    "        'best_metric_value': best_trial.value\n",
    "    })\n",
    "    \n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "def train_final_model(config, train_loader, val_loader):\n",
    "    \"\"\"Train model using parameters from config.\"\"\"\n",
    "    best_model_config = config['best_model']\n",
    "    \n",
    "    final_model = MLPClassifier(\n",
    "        input_size=config['model']['input_size'],\n",
    "        hidden_layers=best_model_config['hidden_layers'],\n",
    "        num_classes=config['model']['num_classes'],\n",
    "        dropout_rate=best_model_config['dropout_rate'],\n",
    "        use_batch_norm=best_model_config['use_batch_norm']\n",
    "    )\n",
    "    \n",
    "    criterion = getattr(nn, config['training']['loss_function'])()\n",
    "    optimizer = getattr(torch.optim, config['training']['optimizer_choice'])(\n",
    "        final_model.parameters(),\n",
    "        lr=best_model_config['learning_rate'],\n",
    "        weight_decay=best_model_config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    final_trainer = PyTorchTrainer(\n",
    "        final_model, criterion, optimizer,\n",
    "        device=config['training']['device'],\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    return final_trainer.train(\n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        config['training']['epochs'],\n",
    "        metric=config['training']['optimization_metric']\n",
    "    )\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def main():\n",
    "    config_path = 'config.yaml'\n",
    "    config = load_config(config_path)\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    set_seed(config['training']['seed'])\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_df = pd.read_csv(config['data']['train_path'])\n",
    "    val_df = pd.read_csv(config['data']['val_path'])\n",
    "    train_dataset = CustomDataset(train_df, config['data']['target_column'])\n",
    "    val_dataset = CustomDataset(val_df, config['data']['target_column'])\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # If best parameters don't exist in config, run hyperparameter tuning\n",
    "    if 'best_model' not in config:\n",
    "        tuner = HyperparameterTuner(config)\n",
    "        best_trial, best_params = tuner.tune(train_loader, val_loader)\n",
    "        save_best_params_to_config(config_path, best_trial, best_params)\n",
    "        # Reload config with saved parameters\n",
    "        config = load_config(config_path)\n",
    "    \n",
    "    print(\"\\nBest model parameters from config:\")\n",
    "    for key, value in config['best_model'].items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    # Restore best model from checkpoint\n",
    "    print(\"\\nRestoring best model from checkpoint...\")\n",
    "    restored = restore_best_model(config)\n",
    "    model = restored['model']\n",
    "    optimizer = restored['optimizer']\n",
    "    \n",
    "    # Create criterion for evaluation\n",
    "    criterion = getattr(nn, config['training']['loss_function'])()\n",
    "    \n",
    "    # Create trainer for evaluation\n",
    "    trainer = PyTorchTrainer(\n",
    "        model, criterion, optimizer,\n",
    "        device=config['training']['device'],\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Evaluate restored model\n",
    "    print(\"\\nEvaluating restored model on validation set...\")\n",
    "    val_loss, val_accuracy, val_f1 = trainer.evaluate(val_loader)\n",
    "    \n",
    "    metric_name = config['training']['optimization_metric']\n",
    "    metric_value = val_f1 if metric_name == 'f1' else val_accuracy\n",
    "    \n",
    "    print(f\"\\nRestored model performance:\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "    print(f\"Validation F1-Score: {val_f1:.4f}\")\n",
    "    print(f\"\\nBest {metric_name.upper()} from tuning: {restored['metric_value']:.4f}\")\n",
    "    print(f\"Current {metric_name.upper()}: {metric_value:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
